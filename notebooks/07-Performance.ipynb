{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avoiding slow code\n",
    "\n",
    "With pandas, you'll get the most bang for your buck by *avoiding antipatterns*.\n",
    "There are additional options like using Numba or Cython if you *really* need to optimize a piece of code, but that's more work typically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will walk through several common miskates, and show more performant ways of achieving the same thing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.options.display.max_rows = 10\n",
    "plt.style.use('default')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mistake 1: Using pandas\n",
    "\n",
    "- At least not for things it's not meant for.\n",
    "- Pandas is very fast at joins, reindex, factorization\n",
    "- Not as great at, say, matrix multiplications or problems that aren't vectorizable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mistake 2: Using object dtype\n",
    "\n",
    "Avoid it if possible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jake VanderPlas has a [great article](https://jakevdp.github.io/blog/2014/05/09/why-python-is-slow/) on why Python is slow for many of the things we care about as analysts / scientists.\n",
    "One reason is the overhead that comes from using python objects for integers, floats, etc. relative to the native versions.\n",
    "\n",
    "As a small demonstration, we'll take two series, one with python integers, and one with NumPy's `int64`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two series of range(10000), different dtypes\n",
    "s1 = pd.Series(range(10000), dtype=object)\n",
    "s2 = pd.Series(range(10000), dtype=np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "886 µs ± 19.4 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit s1.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123 µs ± 4.21 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit s2.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NumPy can process the specialized `int64` dtype array faster than the python object version, even though they're equal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typically you would never expecitly pass in `dtype=object` there, but occasionally object dtypes slip into pandas\n",
    "\n",
    "1. Strings\n",
    "\n",
    "    This is usually unavoidable. Pandas 2 will have a specialized string\n",
    "    type, but for now you're stuck with python objects. If you have few\n",
    "    *distinct* values (relative to the number of rows), you could use a\n",
    "    `Categorical`\n",
    "<br><br>\n",
    "2. Dates, Times\n",
    "\n",
    "    Pandas has implemented a specialized verion of `datetime.datime`,\n",
    "    and `datetime.timedelta`, but not `datetime.date` or `datetime.time`.\n",
    "    Depending on your application, you might be able to treat dates as `datetimes`s, at midnight.\n",
    "<br><br>\n",
    "3. Decimal types\n",
    "\n",
    "    Pandas uses floating-point arrays; there isn't a native arbitrary-precision Decimal type.\n",
    "<br><br>\n",
    "4. Reading messy Excel Files\n",
    "\n",
    "    `read_excel` will preserve the dtype of each cell in the spreadsheet. If you\n",
    "    have a single column with an int, a float, and a datetime, pandas will have to store all of those as `objects`. This dataset probably isn't tidy though.\n",
    "<br><br>\n",
    "\n",
    "5. Messy CSVs where pandas' usual inference fails\n",
    "<br><br>\n",
    "6. Integer NA\n",
    "\n",
    "    Unfortunately, pandas doesn't have real nullable types. To represent\n",
    "    missingness, pandas uses `NaN` (not a number) which is a special floating point value. If you *have* to represent nullable integers, you can use `object` dtype."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1.0\n",
       "1    2.0\n",
       "2    3.0\n",
       "3    NaN\n",
       "4    5.0\n",
       "5    6.0\n",
       "6    7.0\n",
       "7    8.0\n",
       "8    9.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = pd.Series([1, 2, 3, np.nan, 5, 6, 7, 8, 9])\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.float64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(s[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = pd.Series([1, 2, 3, np.nan, 5, 6, 7, 8, 9], dtype=object)\n",
    "type(s[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aside: Managing Dtypes\n",
    "\n",
    "Pandas provides some tools for converting arrays to their specialized dtype.\n",
    "\n",
    "0. IO operations (`read_csv` infers, but can use the `dtype` keyword)\n",
    "1. Object -> numeric: `pd.to_numeric`\n",
    "2. Object -> datetime: `pd.to_datetime`\n",
    "3. Object -> timedelta: `pd.to_timedelta`\n",
    "4. Object -> category: `pd.Categorical`\n",
    "5. `.astype(dtype)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They're pretty flexible what they accept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.,  2.,  3., nan])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numbers = ['1', '2', '3.', 'nan']\n",
    "pd.to_numeric(numbers, errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatetimeIndex(['2016-01-01', '2016-02-01', '2016-03-01'], dtype='datetime64[ns]', freq=None)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dates = ['2016/01/01', '2016/02/01', '2016/03/01']\n",
    "pd.to_datetime(dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TimedeltaIndex(['01:00:00', '00:00:30', NaT], dtype='timedelta64[ns]', freq=None)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to_timedelta\n",
    "x = pd.to_timedelta(['1H', '30s', '20blah'], errors='coerce')\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TimedeltaIndex(['00:10:00', '00:20:00', '00:30:00'], dtype='timedelta64[ns]', freq=None)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.to_timedelta([10, 20, 30], unit=\"T\")  # T = minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[a, a, c, b]\n",
       "Categories (3, object): [a < b < c]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Categorical(['a', 'a', 'c', 'b'], categories=['a', 'b', 'c'],\n",
    "               ordered=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aside: Categoricals\n",
    "\n",
    "Pandas has a custom datatype, `Categorical`, for representing data that can come from a specified, generally fixed set of values.\n",
    "\n",
    "- `categories`: set of valid values\n",
    "- `ordered`: whether that set of values has an ordering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[a, b, c, a]\n",
       "Categories (4, object): [a, b, c, d]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = pd.Categorical(['a', 'b', 'c', 'a'], categories=['a', 'b', 'c', 'd'])\n",
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying to insert a value that is outside the set of categories will cause an exception."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot setitem on a Categorical with a new category, set the categories first",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-0b47dac51aba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'f'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/arrays/categorical.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   1979\u001b[0m         \u001b[0;31m# something to np.nan\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1980\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_add\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_add\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1981\u001b[0;31m             raise ValueError(\"Cannot setitem on a Categorical with a new \"\n\u001b[0m\u001b[1;32m   1982\u001b[0m                              \"category, set the categories first\")\n\u001b[1;32m   1983\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot setitem on a Categorical with a new category, set the categories first"
     ]
    }
   ],
   "source": [
    "c[0] = 'f'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categoricals: Space Efficient\n",
    "\n",
    "Suppose you had to store a column with the home state for 321 million Americans.\n",
    "If you simply stored the text abbreviations like `['AL', 'AL', 'CA', 'IA', ...]` for all 321 million, you'd need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 321_000_000\n",
    "bytes_per = sys.getsizeof(\"AL\")  # two characters per state\n",
    "print(f\"{N * bytes_per:,d} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the other hand, if you store\n",
    "\n",
    "1. Each of the 50 text abbreviations once\n",
    "2. An integer code for each of the 321 million people\n",
    "\n",
    "you'd need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "character_bytes = 50 * sys.getsizeof(\"CA\")\n",
    "bytes_per_person = 2  # np.int16 = 2 bytes\n",
    "print(f\"{character_bytes + N * bytes_per_person:,d} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's what Categoricals do internally. The set of categories [`'a', 'b', 'c', 'd']`, are stored *once*, and is availble with the `.categories` attribute.\n",
    "\n",
    "The values `['a', 'b', 'c', 'a']` aren't actually stored anywhere. Rather, an array of integer `codes` like `[0, 1, 2, 0]`, indiciating the position in `categories` is stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since an integer generally takes less memory to store than a string, these are an efficient way of representing data that has many repetitions. Categoricals can be stored in a DataFrame, Series, or index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas can take shortcuts when you use the proper dtypes.\n",
    "For example, the algorithms for `value_counts` and `groupby` are simplier, and thus faster, for Categoricals than for strings (object).\n",
    "We can time that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10000\n",
    "df = pd.DataFrame({\"A\": np.random.randn(N),\n",
    "                   \"B\": np.random.choice(list('abcdefg'), N)})\n",
    "df['C'] = df[\"B\"].astype(\"category\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# object dtype\n",
    "%timeit df.B.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorical dtype\n",
    "%timeit df.C.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# object dtype\n",
    "%timeit df.groupby(\"B\").A.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorical dtype\n",
    "%timeit df.groupby(\"C\").A.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mistake 3: Initialization\n",
    "\n",
    "When your collecting many different sources (say a bunch of separate CSVs) into\n",
    "a single DataFrame, you have two paths to the same goal:\n",
    "\n",
    "1. Make a single empty DataFrame, append to that\n",
    "2. Make a list of many DataFrames, concat at end\n",
    "\n",
    "Typically, in python we'd choose the first one if we were, for example, collecting things into a `list`. `list.append` is very fast. However `DataFrame.append` is *not* fast."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make some fake datasets to work with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import random\n",
    "\n",
    "records = [[(random.choice(string.ascii_letters),\n",
    "             random.choice(string.ascii_letters),\n",
    "             random.choice(range(10)))\n",
    "            for i in range(50)]\n",
    "           for j in range(100)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have 100 sets of 50 records each.\n",
    "This could represent any datasource, say 100 different CSVs, with any number of items in each.\n",
    "\n",
    "Each subset can be represented as a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(records[0], columns=['A', 'B', 'C'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The wrong way: DataFrame.append"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "# Make an empty dataframe with the correct columns\n",
    "df = pd.DataFrame(columns=['A', 'B', 'C'])\n",
    "\n",
    "for set_ in records:\n",
    "    subdf = pd.DataFrame(set_, columns=['A', 'B', 'C'])\n",
    "    # append to that original df\n",
    "    df = df.append(subdf, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" data-title=\"Concat vs. Append\">\n",
    "  <h1><i class=\"fa fa-tasks\" aria-hidden=\"true\"></i> Exercise: Concat vs. Append</h1>\n",
    "</div>\n",
    "\n",
    "<p>Combine the set of records into a single DataFrame using `pd.concat`</p>\n",
    "\n",
    "Hints:\n",
    "\n",
    "- Make a list of dataframes using a list comprehension\n",
    "- Use `ignore_index=True` in `concat` to avoid duplicaes\n",
    "- add a `%%timeit` magic to time your solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/performance_concat.py\n",
    "\n",
    "pd.concat([pd.DataFrame(set_, columns=['A', 'B', 'C']) for set_ in records],\n",
    "          ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mistake 4: Doing too much work\n",
    "\n",
    "This is more general purpose advice, rather than something you can just grep your code for.\n",
    "But look for places where you're doing a bunch of work, and then throwing some of it away."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in dep_delay column from flights\n",
    "df = pd.read_csv(\"data/ny-flights.csv.gz\", usecols=['dep_delay'])\n",
    "delays = df['dep_delay']\n",
    "delays.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That `read_csv` call unintentionally demonstrates my point. We're only going to use `dep_delay` for this example, so we pass the `usecols` parameter. This means we don't have to do any parsing or type inference on the rest of the columns, since we aren't going to use them.\n",
    "\n",
    "For the real example though, suppose we wanted to find the 5 longest delays. One option is to sort the entire Series and then take the head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit delays.sort_values(ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A better option is to use the `nlargest` method on `Series`, and then sort just those 5 elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit delays.nlargest(5).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In can be hard to remember all the methods or algorithms available to you though.\n",
    "I think this one just comes down to experience.\n",
    "\n",
    "For another example, let's suppose we wanted to find the nearest neighbor for a bunch of points.\n",
    "A naïve implementation would find the ~$N^2$ pairwise distances, and then go through finding the nearest neighbor for each.\n",
    "This becomes untenable for large $N$.\n",
    "Fortunately, we can be more efficient using [KDTrees](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KDTree.html) instead of finding all pairwise distances.\n",
    "For very large $N$, various [probabilistic algorithms](https://github.com/ekzhu/datasketch) are available, but we won't talk about those today."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" data-title=\"Nearest Neighbor\">\n",
    "  <h1><i class=\"fa fa-tasks\" aria-hidden=\"true\"></i> Exercise: Nearest Neighbor</h1>\n",
    "</div>\n",
    "\n",
    "<p>Find the nearest neighbor for all the airports with at least 500 departures.</p>\n",
    "\n",
    "The naïve way to do this is to compute a pairwise distance matrix of all 500 airports, and then lookup the closest neighbor for each airport.\n",
    "\n",
    "Let's avoid that unnescessary computation by using a `KDTree`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: read in `data/flights_coord.csv`. Call the DataFrame coord\n",
    "coord = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load solutions/performance_kd.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I don't know why, but apparently `AIRPORT_ID` isn't unique:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coord.AIRPORT.is_unique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use `.groupby().first()` to arbitrarily select the first one per airport:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locs = coord.groupby('AIRPORT')[['LONGITUDE', 'LATITUDE']].first()\n",
    "locs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also make a mapping between the airport IDs and names for later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = coord.groupby('AIRPORT_ID').AIRPORT.first().dropna()\n",
    "names.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Use [`sklearn.neighbors.KDTree`](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KDTree.html) to find the nearest neighbors.\n",
    "It's OK if you haven't used scikit-learn before; I've provided stubs of a solution below.\n",
    "I'd recommend making additional cell to check the intermediate values as you solve each step.\n",
    "\n",
    "And if you get stuck, the solution is provided afterwards.\n",
    "Feel free to read through it and the KDTree docs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KDTree\n",
    "\n",
    "tree = KDTree(...)  # see the docs (linked above) for KDTree\n",
    "\n",
    "# Now use `tree.query` to find the distances and\n",
    "# nearest neighbor. Make sure to pass a NumPy array to `.query`\n",
    "# and not a DataFrame; otherwise scikit-learn gets confused.\n",
    "# `locs.values` returns a NumPy array.\n",
    "\n",
    "# How many neighbors (k=...) do we need? Keep in mind that\n",
    "# we're searching on the same data as we built the tree on,\n",
    "# so what's the \"closest\" point for each observation?\n",
    "distances, indexes = tree.query(locs.values, k=...)\n",
    "\n",
    "# slice the NumPy arrays to just what we need\n",
    "# The array is shaped [n_observations, k] where `k` is\n",
    "# what you passed to query. Column 0 is the closest,\n",
    "# Column 1 is the second closest, etc...\n",
    "\n",
    "indexes = indexes[:, 1]\n",
    "distances = distances[:, 1]\n",
    "\n",
    "# the result of KDTree.query is a list of index\n",
    "# *positions*, we'll use id_map to go from \n",
    "# positions back to airport names\n",
    "id_map = dict(enumerate(locs.index))\n",
    "\n",
    "neighbors = pd.Series(indexes, index=locs.index).map(id_map)\n",
    "neighbors.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load solutions/performance_02.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mistake 5: Using `.apply` (with axis=1) (Avoid Iteration)\n",
    "\n",
    "I see this one a lot. I don't like absolutes, but you should never use `.apply(..., axis=1)` (probably).\n",
    "The root problem is using for loops instead of a vectorized solution.\n",
    "That is, something like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 2, 4, 6, 8, 10, 12, 14, 16, 18]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = range(10)\n",
    "b = range(10)\n",
    "c = [a_ + b_ for a_, b_ in zip(a, b)]\n",
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of the vectorized version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  2,  4,  6,  8, 10, 12, 14, 16, 18])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.arange(10)\n",
    "b = np.arange(10)\n",
    "c = a + b\n",
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://www.labri.fr/perso/nrougier/from-python-to-numpy/ is a great resource for learning about vectorized methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a demonstration, let's dip into survival analysis.\n",
    "This is (roughly) the study of how much longer something will last (survive) given their current history.\n",
    "For example, how much longer will someone continue to be a customer, given their past purchases?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'lifetimes'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-05bfadc342e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlifetimes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_cdnow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlifetimes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBetaGeoFitter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_cdnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'lifetimes'"
     ]
    }
   ],
   "source": [
    "from lifetimes.datasets import load_cdnow\n",
    "from lifetimes import BetaGeoFitter\n",
    "\n",
    "data = load_cdnow(index_col=[0])\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are all customer-level statistics:\n",
    "\n",
    "- frequency: number of repeat purchases\n",
    "- T: \"age\" of the customer (units since first purchase)\n",
    "- recency: age at time of last purchase\n",
    "\n",
    "The `lifetimes.BetaGeoFitter` model has a scikit-learn-like API to estimate several parameters and has several utility methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# similar API to scikit-learn and lifelines.\n",
    "bgf = BetaGeoFitter(penalizer_coef=0.0)\n",
    "bgf.fit(data['frequency'], data['recency'], data['T']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('default')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lifetimes.plotting import plot_probability_alive_matrix\n",
    "plt.figure(figsize=(12, 10))\n",
    "plot_probability_alive_matrix(bgf, cmap='viridis');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Apparently](http://mktg.uni-svishtov.bg/ivm/resources/Counting_Your_Customers.pdf), we can calculate the expected number of purchases over the next $t$ periods with\n",
    "\n",
    "\\begin{align*}\n",
    "    E\\left(X(t) \\vert \\lambda, p\\right) &= \\lambda t \\cdot P(\\tau > t) + \\int_0^t \\lambda \\tau g(\\tau \\vert \\lambda, p)  \\tau \\\\\n",
    "    &= \\frac{1}{p} - \\frac{1}{p} e^{-\\lambda pt}\n",
    "\\end{align*}\n",
    "\n",
    "With some fancy math and substitutions, the following calculation does that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import special\n",
    "\n",
    "r, α, a, b = bgf._unload_params('r', 'alpha', 'a', 'b')\n",
    "print(r, α, a, b)\n",
    "\n",
    "def conditional_n_purchases(t, frequency, recency, T):\n",
    "    x = frequency\n",
    "\n",
    "    hyp_term = special.hyp2f1(r + x, b + x, a + b + x - 1, t / (α + T + t))\n",
    "    first_term = (a + b + x - 1) / (a - 1)\n",
    "    second_term = (1 - hyp_term * ((α + T) / (α + t + T)) ** (r + x))\n",
    "    numerator = first_term * second_term\n",
    "\n",
    "    denominator = 1 + (x > 0) * (a / (b + x - 1)) * ((α + T) / (α + recency)) ** (r + x)\n",
    "\n",
    "    return numerator / denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditional_n_purchases(t=5, frequency=2, recency=30.43, T=38.86)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Originally, `lifetimes`' documentation had code similar to the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.apply(lambda row: conditional_n_purchases(5,\n",
    "                                               row['frequency'],\n",
    "                                               row['recency'],\n",
    "                                               row['T']),\n",
    "           axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that's doing a `.apply(..., axis=1)`. Let's time it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "t = 5\n",
    "# Using .apply(λ row:, axis=1)\n",
    "data.apply(lambda row: conditional_n_purchases(5,\n",
    "                                               row['frequency'],\n",
    "                                               row['recency'],\n",
    "                                               row['T']),\n",
    "           axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You almost never want to use `.apply` with axis=1. `.apply` It's probably doing way more work that you actually want, including:\n",
    "\n",
    "- A bunch of type inference\n",
    "- Some \"helpful\" reductions of output shape\n",
    "- Is essentially a for loop internally\n",
    "\n",
    "If you *have* to use an interative solution (instead of vectorized), use `.itertuples` which returns an iterator of `namedtuples`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(data.itertuples())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "t = 5\n",
    "pd.Series([\n",
    "    conditional_n_purchases(5, row.frequency, row.recency, row.T)\n",
    "    for row in data.itertuples()\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But, our solution is alread vectorized thanks to NumPy and SciPy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "t = 5\n",
    "# Using vectorization\n",
    "conditional_n_purchases(5, data['frequency'], data['recency'], data['T'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's quite natural to say \"I have this complicated function `f` to apply to each \n",
    "row, I'll just just use `.apply(f, axis=1)`\". Avoid that temptation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "- Get good performance by avoiding antipattern\n",
    "- Avoid `object` dtype where possible\n",
    "- Avoid iteration where possible"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
